# 问题种类：regression 、 classification 、 structuredLearning
$$
y = w * x + b (linear)
$$
* 训练合适的w和b
## 定义loss函数：
* MAE： regression中预测值与label之间差的绝对值和
* MSE： regression中预测值与label之间差的平方和
* Cross-Entropy：classification中预测值与label的交叉熵
## training：找到loss最小的feature
## optimize
* 通常使用梯度下降法：对各个feature求偏导得到梯度，向梯度减小的方向修改参数
* tips：对于斜率变化小的case，在梯度下降的过程中使用相同的步长将很难达到最优的loss。通常使用adam(pytorch自带)、冲量梯度下降法来优化

# DNN

上述linear的方程没法预测更多一般的情况，通常采用多个ReLU函数或者Sigmoid函数来逼近真实的分布
### sigmoid
$$y = b + \sum_i c_i sigmoid(y_j)$$
上式$y_j$也可以写成$y_j = b + \sum_j w_jx_j $
有$$y = b + \sum_i c_i sigmoid(b_i + \sum_j w_{ij}x_i)$$
写成矩阵形式
$$
 y = b + \begin{bmatrix} c\end{bmatrix}^T + sigmoid(b + \begin{bmatrix} W\end{bmatrix}\begin{bmatrix} x\end{bmatrix})
$$
* 上式的$y$当作下一层的输入，又可以套一层神经网路，即是简单的DNN模型
### 批处理
训练时候将数据分成多个batch，每个batch计算出一个loss，根据loss来计算梯度，更新feature。
* 全部batch计算完称为一个epoch

# CNN
CNN通常用于图像操作，输入是一幅图像
## 卷积
对于输入图像看作一个矩阵$\begin{bmatrix} Image\end{bmatrix}$ , 定义一个卷积核矩阵$\begin{bmatrix} Filter\end{bmatrix}$与其上的数字相乘相加得到一个值称为一次卷积  
通过训练出一系列的卷积核一层一层的对图像进行卷积运算提取相关的特征

### 一些问题及解决
* 卷积没法超出边界导致卷积结果大小 < 原图大小 => 通过给图像边界补0（padding）
* 每次卷积都对整个图像操作，计算量大，参数多，容易overfitting => 设定Receptive Field
#### Receptive Field相关
针对不同的特征，课程中提到的“识别鸟”的任务，可以训练一些特征如鸟喙、鸟腿、翅膀。  
将输入图像划分成多个Receptive Field，每个Receptive Field由识别多种特征的神经元来检测。  
相同特征的神经元共享参数，检测不同的Receptive Field（某个特征可能出现在输入图像的不同位置，所以每个Receptive Field都要放检测某一特征的神经元）
#### 池化
有时候运算量过大时，可以使用池化减少运算量
* 池化挑选一次卷积的结果中的某一个值：最大值（max pooling） 最小值（min pooling） 平均值（mean pooling） 
* 但是减少运算量的同时会丢失精度

# self attention
$$
k = W_k * I
$$

$$
q = W_q * I
$$

$$
v = W_v * I
$$
计算第i个输入与第j个输入的相关性
$$
\alpha_{ij} = dot(q_i , k_j )
$$
得到相关性后，该层的输出
$$b_i = \sum_i \alpha_{ij} v_i$$
将得到的结果通过softmax操作得到一个结果的置信值
## multi-head self-attention
self-attention计算 q、k、v, multi-head将计算$q_1,q_2,k_1,k_2,v_1,v_2$  
计算相关性：
$$\alpha_{ijk} = dot(q_{ik} , k_{jk})$$
计算输出：
$$b_{ik} = \sum_i \alpha_{ijk} v_k$$
将得到的$b_i1 , b_i2$拼接：  
$$b_i = \begin{bmatrix} W_b\end{bmatrix} \begin{bmatrix} b_{i1}\\b_{i2}\end{bmatrix}$$
## positional encoding
给输入向量$I$加入位置信息
$$I + e^i$$